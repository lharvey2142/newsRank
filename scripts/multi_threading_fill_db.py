knownrealSites = [    
    'http://www.foxnews.com',
    'https://www.nytimes.com',
    'https://apnews.com',
    'https://ap.org/en-us',
    'http://cnn.com', 
    'https://usnews.com'
    ]
knownFakeSites =['http://www.16WMPO.com', 'http://www.24wpn.com', 'http://www.ABCNews.com.co', 'http://www.actualidadpanam\
ericana.com', 'http://www.AmericanPoliticNews.co', 'http://www.AmericanPresident.co', 'http://www.AMPosts.com', 'http://www\
.ANews24.org/', 'http://www.AngryPatriotMovement.com', 'http://www.Anonjekloy.tk', 'http://www.AssociatedMediaCoverage.com'\
, 'http://www.Aurora-News.us', 'http://www.BB4SP.com', 'http://www.BeforeItsNews.com', 'http://www.BlackInsuranceNews.com',\
 'http://www.BostonTribune.com', 'http://www.BuzzFeedUSA.com', 'http://www.CannaSOS.com', 'http://www.Channel18News.com', '\
http://www.ChristianTimesNewspaper.com', 'http://www.ChristianToday.info', 'http://www.CivicTribune.com', 'http://www.Civic\
Tribune.com', 'http://www.ClashDaily.com', 'http://www.CNNews3.com', 'http://www.Coed.com', 'http://www.ConservativeDailyPo\
st.com', 'http://www.ConservativeFlashNews.com', 'http://www.ConservativeSpirit.com', 'http://www.DailyInfoBox.com', 'http:\
//www.DailyNews10.com', 'http://www.DailyNews5.com', 'http://www.DailyNewsPosts.info', 'http://www.DailySnark.com', 'http:/\
/www.DailySurge.com', 'http://www.DailyUSAUpdate.com', 'http://www.DamnLeaks.com', 'http://www.DepartedMedia.com', 'http://\
www.Disclose.tv', 'http://www.DIYHours.net', 'http://www.DonaldTrumpPOTUS45.com', 'http://www.EmpireHerald.com', 'http://ww\
w.EmpireNews.net', 'http://www.EmpireSports.co', 'http://www.En-Volve.com', 'http://www.ENHLive.com', 'http://www.FedsAlert\
.com', 'http://www.FlashNewsCorner.com', 'http://www.FreedomDaily.com', 'http://www.FreeWoodPost.com', 'http://www.FreshDai\
lyReport.com', 'http://www.GiveMeLiberty01.com', 'http://www.GlobalPoliticsNow.com', 'http://www.GummyPost.com', 'http://ww\
w.HealthyCareAndBeauty.com', 'http://www.HealthyWorldHouse.com', 'http://www.InterestingDailyNews.com',\
, 'http://www.LastDeplorables.com', 'http://www.LearnProgress.org', 'http://www.LiberalPlug.com', 'http://www.LibertyAllian\
ce.com', 'http://www.Local31News.com', 'http://www.MadWorldNews.com', 'http://www.MajorThoughts.com','http://www.Mentor2day.com',\
'http://www.MetropolitanWorlds.com', 'http://www.NationalReport.net', 'http://www.NBC.com.co', 'http://www.NeonNett\
le.com', 'http://www.Nephef.com', 'http://www.NewPoliticsToday.com', 'http://www.News4KTLA.com', 'http://www.NewsBreaksHere\
.com', 'http://www.NewsBySquad.com', 'http://www.NewsDaily12.com', 'http://www.NewsExaminer.net', 'http://www.NewsLeak.co',\
 'http://www.Newslo.com', 'http://www.NewzMagazine.com', 'http://www.NotAllowedTo.com', 'http://www.OccupyDemocrats.com', '\
http://www.OnePoliticalPlaza.com', 'http://www.OpenMagazines.com', 'http://www.Politicalo.com',\
'http://www.Politicono.com', 'http://www.Politicops.com', 'http://www.Politicot.com', 'http://www.PoliticsUSANews.com', 'http://www.President45DonaldTrump.com',\
'http://www.Prntly.com', 'http://www.RedCountry.us', 'http://www.RedRockTribune.com', 'http://www.Religionlo.com', 'http://www.ReligionMind.com', 'http://www.Rogue-Nation3.com', 'http://www.RumorJournal.com', 'http://www.SatiraTribune.com', 'http://www.Smag31.com',\
'http://www.SocialEverythings.com', 'http://www.Success-Street.com', 'http://www.SupremePatr\
iot.com', 'http://www.TDTAlliance.com', 'http://www.TeaParty.org', 'http://www.ThatViralFeed.net', 'http://www.The-Insider.\
co', 'http://www.TheBigRiddle.com', 'http://www.TheInternetPost.net', 'http://www.TheLastLineOfDefense.org', 'http://www.Th\
eMoralOfTheStory.us', 'http://www.TheNationalMarijuanaNews.com', 'http://www.TheNet24h.com', 'http://www.TheNewYorkEvening.\
com', 'http://www.ThePoliticalInsider.com', 'http://www.TheRightists.com', 'http://www.TheSeattleTribune.com', 'http://www.\
TheTrumpMedia.com', 'http://www.TheUSA-News.com', 'http://www.TheWashingtonPress.com', 'http://www.Times.com.mx', 'http://w\
ww.TMZWorldNews.com', 'http://www.TrueAmericans.me', 'http://www.TrueTrumpers.com', 'http://www.UndergroundNewsReport.com',\
 'http://www.UniversePolitics.com', 'http://www.UrbanImageMagazine.com', 'http://www.USA-Radio.com', 'http://www.USA-Televi\
sion.com', 'http://www.USADailyInfo.com', 'http://www.USADailyPost.us', 'http://www.USADailyTime.com', 'http://www.USADoseN\
ews.com', 'http://www.USAFirstInformation.com', 'http://www.USANewsToday.com', 'http://www.USAPolitics24hrs.com', 'http://w\
ww.USAPoliticsToday.com', 'http://www.USAPoliticsZone.com', 'http://www.USASnich.com', 'http://www.USATodayNews.me', 'http:\
//www.USHealthyAdvisor.com', 'http://www.USHealthyLife.com', 'http://www.USHerald.com', 'http://www.USInfoNews.com', 'http:\
//www.USPOLN.com', 'http://www.USPostman.com', 'http://www.ViralActions.com', 'http://www.VoxTribune.com', 'http://www.Wash\
ingtonFeed.com', 'http://www.WashingtonPost.com.co', 'http://www.WorldNewsDailyReport.com', 'http://www.WorldPoliticsNow.com']

import sys
sys.path.append('/Users/froyvalencia/Desktop/newsRank')
import os
os.environ.setdefault('DJANGO_SETTINGS_MODULE','mysite.settings')
import django
django.setup()

from django.contrib.auth.models import User
users = User.objects.all()

import newspaper
from newspaper import news_pool
import nltk
import tweetParser
from newsApp.models import Article

def loadNews(knownrealSites):    
    articles = Article.objects.all()
    #for a in articles:
    #print(a)
    papers = []
    for url in knownrealSites:
        real_paper = None
        try:
            real_paper = newspaper.build(url)
            papers.append(real_paper)
            print(url + ' contains ' + str(len(real_paper.articles)) + ' real articles')
        except: 
            print(url)
            print('url is bad')
            continue
    news_pool.set(papers, threads_per_source=3)
    news_pool.join()
    for paper in papers:
        for article in paper.articles:
            #due to multithreading above we can assume every article has had download called on it.
            #for article in real_paper.articles:
            try:
                #article.download()
                article.parse()
                #print('article.authors:**************************\n');print(article.authors)
                #print('article.text:**************************\n');print(article.text)
                #print('article.url:**************************\n');print(article.url)
                #print('article.title:**************************\n');print(article.title)
                #article.nlp()
                #print('keywords:**************************\n');print(article.keywords)
                #print('summary:**************************\n');print(article.summary)
            except:
                print('issue with download/parse')
                continue
            #x,y,z = tweetParser.getSentiment(url,2000)
            #print(article.publish_date)
            a = Article(
                address = article.url,
                title = article.title,
                body = article.text,
                date = article.publish_date,
                result = 'reliable',
                #positive = x,
                #negative = y,
                #neutral = z,
                )
            try:
                a.save()
                print('**************************article SAVED**************************')
            except:
                print('**************** article failed to save with field **************')
                continue
if __name__ == "__main__":
    loadNews(knownrealSites)